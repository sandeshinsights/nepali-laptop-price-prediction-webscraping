{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMviN7igUn9IJGQpZyP5rYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeshinsights/nepali-laptop-price-prediction-webscraping/blob/main/Scraping_Laptop_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "url = \"https://infotechsnepal.com.np/_next/data/hDb-Tv_8KG32-7-LaIGy_/laptops.json\"\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "# Correct path to products\n",
        "products = data.get(\"pageProps\", {}).get(\"productCat\", [])\n",
        "\n",
        "output_file = \"infotechs_laptop_prices.csv\"\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Name\", \"Price (NPR)\", \"Product URL\"])  # CSV header\n",
        "\n",
        "    for product in products:\n",
        "        name = product.get(\"name\", \"N/A\")\n",
        "        price = product.get(\"price\", \"N/A\")\n",
        "        url = product.get(\"permalink\") or product.get(\"product_url\", \"N/A\")\n",
        "        writer.writerow([name, price, url])\n",
        "\n",
        "print(f\"Saved {len(products)} laptop prices to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgVqnqgCpvFQ",
        "outputId": "8e20819e-8c2a-4270-a0ac-624af39fec57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 732 laptop prices to infotechs_laptop_prices.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://infotechsnepal.com.np/laptops/?page={}\"\n",
        "\n",
        "# Output CSV file\n",
        "csv_file = \"laptops.csv\"\n",
        "\n",
        "# Headers for HTTP request (to mimic a browser)\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Open CSV for writing\n",
        "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Laptop Name\", \"Price (NPR)\"])\n",
        "\n",
        "    for page in range(1, 38):  # Pages 1 to 37\n",
        "        url = base_url.format(page)\n",
        "        print(f\"Fetching page {page}...\")\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract names & prices\n",
        "        names = soup.find_all(\"p\", class_=\"font-semibold text-[14px] color-[#212529] line-clamp-4\")\n",
        "        prices = soup.find_all(\"span\", class_=\"woocommerce-Price-amount amount\")\n",
        "\n",
        "        for name, price in zip(names, prices):\n",
        "            price_text = price.get_text(strip=True)\n",
        "            name_text = name.get_text(strip=True)\n",
        "            writer.writerow([name_text, price_text])\n",
        "\n",
        "        time.sleep(1)  # Be polite and avoid hammering the server\n",
        "\n",
        "print(f\"Scraping completed. Data saved in {csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_Zgnfjr0-iV",
        "outputId": "e1f78a1b-a813-49f7-d36d-25fb754fa195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Fetching page 5...\n",
            "Fetching page 6...\n",
            "Fetching page 7...\n",
            "Fetching page 8...\n",
            "Fetching page 9...\n",
            "Fetching page 10...\n",
            "Fetching page 11...\n",
            "Fetching page 12...\n",
            "Fetching page 13...\n",
            "Fetching page 14...\n",
            "Fetching page 15...\n",
            "Fetching page 16...\n",
            "Fetching page 17...\n",
            "Fetching page 18...\n",
            "Fetching page 19...\n",
            "Fetching page 20...\n",
            "Fetching page 21...\n",
            "Fetching page 22...\n",
            "Fetching page 23...\n",
            "Fetching page 24...\n",
            "Fetching page 25...\n",
            "Fetching page 26...\n",
            "Fetching page 27...\n",
            "Fetching page 28...\n",
            "Fetching page 29...\n",
            "Fetching page 30...\n",
            "Fetching page 31...\n",
            "Fetching page 32...\n",
            "Fetching page 33...\n",
            "Fetching page 34...\n",
            "Fetching page 35...\n",
            "Fetching page 36...\n",
            "Fetching page 37...\n",
            "Scraping completed. Data saved in laptops.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLyXS-Rz4lFC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}